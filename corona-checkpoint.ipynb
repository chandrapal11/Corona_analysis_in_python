{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'C:/Users/Chandrapal Panwar/Desktop/python/covid-19/biorxiv_medrxiv/biorxiv_medrxiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n",
    "                  \"abstract\": [None], \"text_body\": [None]}\n",
    "corona_df = pd.DataFrame.from_dict(corona_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_id source title abstract text_body\n",
       "0   None   None  None     None      None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_corona_df(json_filenames, df, source):\n",
    "\n",
    "    for file_name in json_filenames:\n",
    "\n",
    "        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n",
    "              \"abstract\": None, \"text_body\": None}\n",
    "\n",
    "        with open(file_name) as json_data:\n",
    "            data = json.load(json_data)\n",
    "\n",
    "            doc_id = data['paper_id']\n",
    "            row['doc_id'] = doc_id\n",
    "            row['title'] = data['metadata']['title']\n",
    "\n",
    "            # Now need all of abstract. Put it all in \n",
    "            # a list then use str.join() to split it\n",
    "            # into paragraphs. \n",
    "\n",
    "            abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "            abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "            row['abstract'] = abstract\n",
    "\n",
    "            # And lastly the body of the text. \n",
    "            body_list = [bt['text'] for bt in data['body_text']]\n",
    "            body = \"\\n \".join(body_list)\n",
    "            \n",
    "            row['text_body'] = body\n",
    "            \n",
    "            # Now just add to the dataframe. \n",
    "            \n",
    "            if source == 'b':\n",
    "                row['source'] = \"BIORXIV\"\n",
    "            elif source == \"c\":\n",
    "                row['source'] = \"COMMON_USE_SUB\"\n",
    "            elif source == \"n\":\n",
    "                row['source'] = \"NON_COMMON_USE\"\n",
    "            elif source == \"p\":\n",
    "                row['source'] = \"PMC_CUSTOM_LICENSE\"\n",
    "            \n",
    "            df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df = return_corona_df(json_filenames, corona_df, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>The RNA pseudoknots in foot-and-mouth disease ...</td>\n",
       "      <td>word count: 194 22 Text word count: 5168 23 24...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Healthcare-resource-adjusted vulnerabilities t...</td>\n",
       "      <td></td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Real-time, MinION-based, amplicon sequencing f...</td>\n",
       "      <td>Infectious bronchitis (IB) causes significant ...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0139ea4ca580af99b602c6435368e7fdbefacb03</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>A Combined Evidence Approach to Prioritize Nip...</td>\n",
       "      <td>Nipah Virus (NiV) came into limelight recently...</td>\n",
       "      <td>Nipah is an infectious negative-sense single-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id   source  \\\n",
       "0                                      None     None   \n",
       "1  0015023cc06b5362d332b3baf348d11567ca2fbb  BIORXIV   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76  BIORXIV   \n",
       "3  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b  BIORXIV   \n",
       "4  0139ea4ca580af99b602c6435368e7fdbefacb03  BIORXIV   \n",
       "\n",
       "                                               title  \\\n",
       "0                                               None   \n",
       "1  The RNA pseudoknots in foot-and-mouth disease ...   \n",
       "2  Healthcare-resource-adjusted vulnerabilities t...   \n",
       "3  Real-time, MinION-based, amplicon sequencing f...   \n",
       "4  A Combined Evidence Approach to Prioritize Nip...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0                                               None   \n",
       "1  word count: 194 22 Text word count: 5168 23 24...   \n",
       "2                                                      \n",
       "3  Infectious bronchitis (IB) causes significant ...   \n",
       "4  Nipah Virus (NiV) came into limelight recently...   \n",
       "\n",
       "                                           text_body  \n",
       "0                                               None  \n",
       "1  VP3, and VP0 (which is further processed to VP...  \n",
       "2  The 2019-nCoV epidemic has spread across China...  \n",
       "3  Infectious bronchitis (IB), which is caused by...  \n",
       "4  Nipah is an infectious negative-sense single-s...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df = corona_df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>The RNA pseudoknots in foot-and-mouth disease ...</td>\n",
       "      <td>word count: 194 22 Text word count: 5168 23 24...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Healthcare-resource-adjusted vulnerabilities t...</td>\n",
       "      <td></td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Real-time, MinION-based, amplicon sequencing f...</td>\n",
       "      <td>Infectious bronchitis (IB) causes significant ...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0139ea4ca580af99b602c6435368e7fdbefacb03</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>A Combined Evidence Approach to Prioritize Nip...</td>\n",
       "      <td>Nipah Virus (NiV) came into limelight recently...</td>\n",
       "      <td>Nipah is an infectious negative-sense single-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>013d9d1cba8a54d5d3718c229b812d7cf91b6c89</td>\n",
       "      <td>BIORXIV</td>\n",
       "      <td>Assessing spread risk of Wuhan novel coronavir...</td>\n",
       "      <td>Background: A novel coronavirus (2019-nCoV) em...</td>\n",
       "      <td>In December 2019, a cluster of patients with p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id   source  \\\n",
       "1  0015023cc06b5362d332b3baf348d11567ca2fbb  BIORXIV   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76  BIORXIV   \n",
       "3  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b  BIORXIV   \n",
       "4  0139ea4ca580af99b602c6435368e7fdbefacb03  BIORXIV   \n",
       "5  013d9d1cba8a54d5d3718c229b812d7cf91b6c89  BIORXIV   \n",
       "\n",
       "                                               title  \\\n",
       "1  The RNA pseudoknots in foot-and-mouth disease ...   \n",
       "2  Healthcare-resource-adjusted vulnerabilities t...   \n",
       "3  Real-time, MinION-based, amplicon sequencing f...   \n",
       "4  A Combined Evidence Approach to Prioritize Nip...   \n",
       "5  Assessing spread risk of Wuhan novel coronavir...   \n",
       "\n",
       "                                            abstract  \\\n",
       "1  word count: 194 22 Text word count: 5168 23 24...   \n",
       "2                                                      \n",
       "3  Infectious bronchitis (IB) causes significant ...   \n",
       "4  Nipah Virus (NiV) came into limelight recently...   \n",
       "5  Background: A novel coronavirus (2019-nCoV) em...   \n",
       "\n",
       "                                           text_body  \n",
       "1  VP3, and VP0 (which is further processed to VP...  \n",
       "2  The 2019-nCoV epidemic has spread across China...  \n",
       "3  Infectious bronchitis (IB), which is caused by...  \n",
       "4  Nipah is an infectious negative-sense single-s...  \n",
       "5  In December 2019, a cluster of patients with p...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corona_out = corona_df.to_csv('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/kaggle_covid-19_open_csv_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=corona_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      the rna pseudoknots in foot-and-mouth disease ...\n",
       "2      healthcare-resource-adjusted vulnerabilities t...\n",
       "3      real-time, minion-based, amplicon sequencing f...\n",
       "4      a combined evidence approach to prioritize nip...\n",
       "5      assessing spread risk of wuhan novel coronavir...\n",
       "                             ...                        \n",
       "881    deep sequencing of primary human lung epitheli...\n",
       "882    increased neurite orientation-dispersion and d...\n",
       "883    title: estimation of covid-2019 burden and pot...\n",
       "884    planning horizon affects prophylactic decision...\n",
       "885    nucleotide analogues as inhibitors of viral po...\n",
       "Name: title, Length: 885, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[df['title'].str.contains('cor')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(['doc_id','source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Assessing spread risk of Wuhan novel coronavir...</td>\n",
       "      <td>Background: A novel coronavirus (2019-nCoV) em...</td>\n",
       "      <td>In December 2019, a cluster of patients with p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>TWIRLS, an automated topic-wise inference meth...</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Estimation of the final size of the second pha...</td>\n",
       "      <td>In the note, the logistic growth regression mo...</td>\n",
       "      <td>however, use approximate solution and thus obt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Development of CRISPR as a prophylactic strate...</td>\n",
       "      <td></td>\n",
       "      <td>The world is currently faced with a pandemic o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Mycroft-West et al. (2020) Running title: SARS...</td>\n",
       "      <td>Many pathogens take advantage of the dependenc...</td>\n",
       "      <td>Heparin, the second most widely used drug by w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>Title: ACP risk grade: a simple mortality inde...</td>\n",
       "      <td>Since the severe acute respiratory syndrome co...</td>\n",
       "      <td>Background: Since the severe acute respiratory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>Trypsin treatment unlocks barrier for zoonotic...</td>\n",
       "      <td>Traditionally, the emergence of coronaviruses ...</td>\n",
       "      <td>Since the beginning of the 21 st century, publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>Transmission of corona virus disease 2019 duri...</td>\n",
       "      <td>The ongoing outbreak of novel corona virus dis...</td>\n",
       "      <td>The corona virus disease has spread rapidly th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>Modelling the epidemic trend of the 2019 novel...</td>\n",
       "      <td></td>\n",
       "      <td>On 12 th December 2019, a pneumonia case of un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>Title: Antibody responses to SARS-CoV-2 in pat...</td>\n",
       "      <td>The novel coronavirus SARS-CoV-2 is a newly em...</td>\n",
       "      <td>Since early December of 2019 and up to Februar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "5    Assessing spread risk of Wuhan novel coronavir...   \n",
       "6    TWIRLS, an automated topic-wise inference meth...   \n",
       "14   Estimation of the final size of the second pha...   \n",
       "16   Development of CRISPR as a prophylactic strate...   \n",
       "22   Mycroft-West et al. (2020) Running title: SARS...   \n",
       "..                                                 ...   \n",
       "845  Title: ACP risk grade: a simple mortality inde...   \n",
       "846  Trypsin treatment unlocks barrier for zoonotic...   \n",
       "871  Transmission of corona virus disease 2019 duri...   \n",
       "875  Modelling the epidemic trend of the 2019 novel...   \n",
       "879  Title: Antibody responses to SARS-CoV-2 in pat...   \n",
       "\n",
       "                                              abstract  \\\n",
       "5    Background: A novel coronavirus (2019-nCoV) em...   \n",
       "6    Faced with the current large-scale public heal...   \n",
       "14   In the note, the logistic growth regression mo...   \n",
       "16                                                       \n",
       "22   Many pathogens take advantage of the dependenc...   \n",
       "..                                                 ...   \n",
       "845  Since the severe acute respiratory syndrome co...   \n",
       "846  Traditionally, the emergence of coronaviruses ...   \n",
       "871  The ongoing outbreak of novel corona virus dis...   \n",
       "875                                                      \n",
       "879  The novel coronavirus SARS-CoV-2 is a newly em...   \n",
       "\n",
       "                                             text_body  \n",
       "5    In December 2019, a cluster of patients with p...  \n",
       "6    The sudden outbreak of the new coronavirus (SA...  \n",
       "14   however, use approximate solution and thus obt...  \n",
       "16   The world is currently faced with a pandemic o...  \n",
       "22   Heparin, the second most widely used drug by w...  \n",
       "..                                                 ...  \n",
       "845  Background: Since the severe acute respiratory...  \n",
       "846  Since the beginning of the 21 st century, publ...  \n",
       "871  The corona virus disease has spread rapidly th...  \n",
       "875  On 12 th December 2019, a pneumonia case of un...  \n",
       "879  Since early December of 2019 and up to Februar...  \n",
       "\n",
       "[124 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treat=pd.DataFrame()\n",
    "data_treat=df1[df1['text_body'].str.contains(\"treat\")]\n",
    "data_treat\n",
    "corona_treat =data_treat.to_csv('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/treatment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_virus=pd.DataFrame()\n",
    "data_virus=df1[df1['text_body'].str.contains(\"virus\")]\n",
    "data_virus\n",
    "corona_virus =data_virus.to_csv('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/virus_data.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_symptom=pd.DataFrame()\n",
    "data_symptom=df1[df1['text_body'].str.contains(\"symptom\")]\n",
    "data_symptom\n",
    "corona_symptom =data_symptom.to_csv('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/symptom_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>TWIRLS, an automated topic-wise inference meth...</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "      <td>[The, sudden, outbreak, of, the, new, coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Development of CRISPR as a prophylactic strate...</td>\n",
       "      <td></td>\n",
       "      <td>The world is currently faced with a pandemic o...</td>\n",
       "      <td>[The, world, is, currently, faced, with, a, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Mycroft-West et al. (2020) Running title: SARS...</td>\n",
       "      <td>Many pathogens take advantage of the dependenc...</td>\n",
       "      <td>Heparin, the second most widely used drug by w...</td>\n",
       "      <td>[Heparin, ,, the, second, most, widely, used, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>Reconciling early-outbreak estimates of the ba...</td>\n",
       "      <td>A novel coronavirus (SARS-CoV-2) has recently ...</td>\n",
       "      <td>Since December 2019, a novel coronavirus (SARS...</td>\n",
       "      <td>[Since, December, 2019, ,, a, novel, coronavir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>Characteristics of lymphocyte subsets and cyto...</td>\n",
       "      <td>Background: To explore the cellular immunity a...</td>\n",
       "      <td>CC-BY-NC 4.0 International license It is made ...</td>\n",
       "      <td>[CC-BY-NC, 4.0, International, license, It, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>Multi-city modeling of epidemics using spatial...</td>\n",
       "      <td>The ongoing pandemic of 2019-nCov (COVID-19) c...</td>\n",
       "      <td>Following its onset in Wuhan, China, the pande...</td>\n",
       "      <td>[Following, its, onset, in, Wuhan, ,, China, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>Proofreading-deficient coronaviruses adapt ove...</td>\n",
       "      <td>The coronavirus (CoV) RNA genome is the larges...</td>\n",
       "      <td>9 also deleted ORF 4a, which is dispensable fo...</td>\n",
       "      <td>[9, also, deleted, ORF, 4a, ,, which, is, disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>Title: ACP risk grade: a simple mortality inde...</td>\n",
       "      <td>Since the severe acute respiratory syndrome co...</td>\n",
       "      <td>Background: Since the severe acute respiratory...</td>\n",
       "      <td>[Background, :, Since, the, severe, acute, res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>Trypsin treatment unlocks barrier for zoonotic...</td>\n",
       "      <td>Traditionally, the emergence of coronaviruses ...</td>\n",
       "      <td>Since the beginning of the 21 st century, publ...</td>\n",
       "      <td>[Since, the, beginning, of, the, 21, st, centu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>Title: Antibody responses to SARS-CoV-2 in pat...</td>\n",
       "      <td>The novel coronavirus SARS-CoV-2 is a newly em...</td>\n",
       "      <td>Since early December of 2019 and up to Februar...</td>\n",
       "      <td>[Since, early, December, of, 2019, and, up, to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "6    TWIRLS, an automated topic-wise inference meth...   \n",
       "16   Development of CRISPR as a prophylactic strate...   \n",
       "22   Mycroft-West et al. (2020) Running title: SARS...   \n",
       "52   Reconciling early-outbreak estimates of the ba...   \n",
       "76   Characteristics of lymphocyte subsets and cyto...   \n",
       "..                                                 ...   \n",
       "805  Multi-city modeling of epidemics using spatial...   \n",
       "816  Proofreading-deficient coronaviruses adapt ove...   \n",
       "845  Title: ACP risk grade: a simple mortality inde...   \n",
       "846  Trypsin treatment unlocks barrier for zoonotic...   \n",
       "879  Title: Antibody responses to SARS-CoV-2 in pat...   \n",
       "\n",
       "                                              abstract  \\\n",
       "6    Faced with the current large-scale public heal...   \n",
       "16                                                       \n",
       "22   Many pathogens take advantage of the dependenc...   \n",
       "52   A novel coronavirus (SARS-CoV-2) has recently ...   \n",
       "76   Background: To explore the cellular immunity a...   \n",
       "..                                                 ...   \n",
       "805  The ongoing pandemic of 2019-nCov (COVID-19) c...   \n",
       "816  The coronavirus (CoV) RNA genome is the larges...   \n",
       "845  Since the severe acute respiratory syndrome co...   \n",
       "846  Traditionally, the emergence of coronaviruses ...   \n",
       "879  The novel coronavirus SARS-CoV-2 is a newly em...   \n",
       "\n",
       "                                             text_body  \\\n",
       "6    The sudden outbreak of the new coronavirus (SA...   \n",
       "16   The world is currently faced with a pandemic o...   \n",
       "22   Heparin, the second most widely used drug by w...   \n",
       "52   Since December 2019, a novel coronavirus (SARS...   \n",
       "76   CC-BY-NC 4.0 International license It is made ...   \n",
       "..                                                 ...   \n",
       "805  Following its onset in Wuhan, China, the pande...   \n",
       "816  9 also deleted ORF 4a, which is dispensable fo...   \n",
       "845  Background: Since the severe acute respiratory...   \n",
       "846  Since the beginning of the 21 st century, publ...   \n",
       "879  Since early December of 2019 and up to Februar...   \n",
       "\n",
       "                                         tokenize_text  \n",
       "6    [The, sudden, outbreak, of, the, new, coronavi...  \n",
       "16   [The, world, is, currently, faced, with, a, pa...  \n",
       "22   [Heparin, ,, the, second, most, widely, used, ...  \n",
       "52   [Since, December, 2019, ,, a, novel, coronavir...  \n",
       "76   [CC-BY-NC, 4.0, International, license, It, is...  \n",
       "..                                                 ...  \n",
       "805  [Following, its, onset, in, Wuhan, ,, China, ,...  \n",
       "816  [9, also, deleted, ORF, 4a, ,, which, is, disp...  \n",
       "845  [Background, :, Since, the, severe, acute, res...  \n",
       "846  [Since, the, beginning, of, the, 21, st, centu...  \n",
       "879  [Since, early, December, of, 2019, and, up, to...  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=data_treat\n",
    "############ Tokenizing the text_body\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tokenize_text'] = df['text_body'].apply(word_tokenize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>tokenize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>TWIRLS, an automated topic-wise inference meth...</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "      <td>[The, sudden, outbreak, of, the, new, coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Development of CRISPR as a prophylactic strate...</td>\n",
       "      <td></td>\n",
       "      <td>The world is currently faced with a pandemic o...</td>\n",
       "      <td>[The, world, is, currently, faced, with, a, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Mycroft-West et al. (2020) Running title: SARS...</td>\n",
       "      <td>Many pathogens take advantage of the dependenc...</td>\n",
       "      <td>Heparin, the second most widely used drug by w...</td>\n",
       "      <td>[Heparin, ,, the, second, most, widely, used, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>Reconciling early-outbreak estimates of the ba...</td>\n",
       "      <td>A novel coronavirus (SARS-CoV-2) has recently ...</td>\n",
       "      <td>Since December 2019, a novel coronavirus (SARS...</td>\n",
       "      <td>[Since, December, ,, a, novel, coronavirus, (,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>Characteristics of lymphocyte subsets and cyto...</td>\n",
       "      <td>Background: To explore the cellular immunity a...</td>\n",
       "      <td>CC-BY-NC 4.0 International license It is made ...</td>\n",
       "      <td>[CC-BY-NC, International, license, It, is, mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>Multi-city modeling of epidemics using spatial...</td>\n",
       "      <td>The ongoing pandemic of 2019-nCov (COVID-19) c...</td>\n",
       "      <td>Following its onset in Wuhan, China, the pande...</td>\n",
       "      <td>[Following, its, onset, in, Wuhan, ,, China, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>Proofreading-deficient coronaviruses adapt ove...</td>\n",
       "      <td>The coronavirus (CoV) RNA genome is the larges...</td>\n",
       "      <td>9 also deleted ORF 4a, which is dispensable fo...</td>\n",
       "      <td>[also, deleted, ORF, ,, which, is, dispensable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>Title: ACP risk grade: a simple mortality inde...</td>\n",
       "      <td>Since the severe acute respiratory syndrome co...</td>\n",
       "      <td>Background: Since the severe acute respiratory...</td>\n",
       "      <td>[Background, :, Since, the, severe, acute, res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>Trypsin treatment unlocks barrier for zoonotic...</td>\n",
       "      <td>Traditionally, the emergence of coronaviruses ...</td>\n",
       "      <td>Since the beginning of the 21 st century, publ...</td>\n",
       "      <td>[Since, the, beginning, of, the, st, century, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>Title: Antibody responses to SARS-CoV-2 in pat...</td>\n",
       "      <td>The novel coronavirus SARS-CoV-2 is a newly em...</td>\n",
       "      <td>Since early December of 2019 and up to Februar...</td>\n",
       "      <td>[Since, early, December, of, and, up, to, Febr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "6    TWIRLS, an automated topic-wise inference meth...   \n",
       "16   Development of CRISPR as a prophylactic strate...   \n",
       "22   Mycroft-West et al. (2020) Running title: SARS...   \n",
       "52   Reconciling early-outbreak estimates of the ba...   \n",
       "76   Characteristics of lymphocyte subsets and cyto...   \n",
       "..                                                 ...   \n",
       "805  Multi-city modeling of epidemics using spatial...   \n",
       "816  Proofreading-deficient coronaviruses adapt ove...   \n",
       "845  Title: ACP risk grade: a simple mortality inde...   \n",
       "846  Trypsin treatment unlocks barrier for zoonotic...   \n",
       "879  Title: Antibody responses to SARS-CoV-2 in pat...   \n",
       "\n",
       "                                              abstract  \\\n",
       "6    Faced with the current large-scale public heal...   \n",
       "16                                                       \n",
       "22   Many pathogens take advantage of the dependenc...   \n",
       "52   A novel coronavirus (SARS-CoV-2) has recently ...   \n",
       "76   Background: To explore the cellular immunity a...   \n",
       "..                                                 ...   \n",
       "805  The ongoing pandemic of 2019-nCov (COVID-19) c...   \n",
       "816  The coronavirus (CoV) RNA genome is the larges...   \n",
       "845  Since the severe acute respiratory syndrome co...   \n",
       "846  Traditionally, the emergence of coronaviruses ...   \n",
       "879  The novel coronavirus SARS-CoV-2 is a newly em...   \n",
       "\n",
       "                                             text_body  \\\n",
       "6    The sudden outbreak of the new coronavirus (SA...   \n",
       "16   The world is currently faced with a pandemic o...   \n",
       "22   Heparin, the second most widely used drug by w...   \n",
       "52   Since December 2019, a novel coronavirus (SARS...   \n",
       "76   CC-BY-NC 4.0 International license It is made ...   \n",
       "..                                                 ...   \n",
       "805  Following its onset in Wuhan, China, the pande...   \n",
       "816  9 also deleted ORF 4a, which is dispensable fo...   \n",
       "845  Background: Since the severe acute respiratory...   \n",
       "846  Since the beginning of the 21 st century, publ...   \n",
       "879  Since early December of 2019 and up to Februar...   \n",
       "\n",
       "                                         tokenize_text  \n",
       "6    [The, sudden, outbreak, of, the, new, coronavi...  \n",
       "16   [The, world, is, currently, faced, with, a, pa...  \n",
       "22   [Heparin, ,, the, second, most, widely, used, ...  \n",
       "52   [Since, December, ,, a, novel, coronavirus, (,...  \n",
       "76   [CC-BY-NC, International, license, It, is, mad...  \n",
       "..                                                 ...  \n",
       "805  [Following, its, onset, in, Wuhan, ,, China, ,...  \n",
       "816  [also, deleted, ORF, ,, which, is, dispensable...  \n",
       "845  [Background, :, Since, the, severe, acute, res...  \n",
       "846  [Since, the, beginning, of, the, st, century, ...  \n",
       "879  [Since, early, December, of, and, up, to, Febr...  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if a word has a digit, remove that word\n",
    "df['tokenize_text']=df['tokenize_text'].apply(lambda x: [y for y in x if not any (c.isdigit() for c in y)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(df['tokenize_text'], workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "######## Finding vector corresponding to each text\n",
    "import numpy as np\n",
    "vocab = list(model.wv.vocab)\n",
    "def sentence_vector(sentence, model):\n",
    "    nwords = 0\n",
    "    featureV = np.zeros(100, dtype=\"float32\")\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        featureV = np.add(featureV, model[word])\n",
    "        nwords = nwords + 1\n",
    "    if nwords > 0: \n",
    "        featureV = np.divide(featureV, nwords)\n",
    "    return featureV\n",
    "\n",
    "text_vector = df['tokenize_text'].apply(lambda x: sentence_vector(x, model))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071635</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.185331</td>\n",
       "      <td>0.038864</td>\n",
       "      <td>0.023363</td>\n",
       "      <td>-0.053904</td>\n",
       "      <td>-0.064196</td>\n",
       "      <td>0.027372</td>\n",
       "      <td>-0.132629</td>\n",
       "      <td>-0.014867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100373</td>\n",
       "      <td>-0.114019</td>\n",
       "      <td>-0.163803</td>\n",
       "      <td>-0.085835</td>\n",
       "      <td>0.169530</td>\n",
       "      <td>-0.007652</td>\n",
       "      <td>-0.070269</td>\n",
       "      <td>0.064188</td>\n",
       "      <td>-0.017544</td>\n",
       "      <td>0.005122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.059997</td>\n",
       "      <td>-0.004085</td>\n",
       "      <td>0.173721</td>\n",
       "      <td>0.044702</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>-0.051654</td>\n",
       "      <td>-0.065923</td>\n",
       "      <td>0.023795</td>\n",
       "      <td>-0.110264</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096251</td>\n",
       "      <td>-0.113163</td>\n",
       "      <td>-0.170624</td>\n",
       "      <td>-0.080046</td>\n",
       "      <td>0.159477</td>\n",
       "      <td>-0.006373</td>\n",
       "      <td>-0.070414</td>\n",
       "      <td>0.057019</td>\n",
       "      <td>-0.022347</td>\n",
       "      <td>0.010198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.054114</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>0.185281</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.029967</td>\n",
       "      <td>-0.050980</td>\n",
       "      <td>-0.054634</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>-0.114730</td>\n",
       "      <td>-0.004722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109594</td>\n",
       "      <td>-0.115894</td>\n",
       "      <td>-0.168658</td>\n",
       "      <td>-0.075294</td>\n",
       "      <td>0.177010</td>\n",
       "      <td>-0.008734</td>\n",
       "      <td>-0.062928</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>-0.029788</td>\n",
       "      <td>0.010018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.056961</td>\n",
       "      <td>-0.003406</td>\n",
       "      <td>0.171711</td>\n",
       "      <td>0.048147</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>-0.050351</td>\n",
       "      <td>-0.070991</td>\n",
       "      <td>0.018756</td>\n",
       "      <td>-0.100210</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104847</td>\n",
       "      <td>-0.130948</td>\n",
       "      <td>-0.164393</td>\n",
       "      <td>-0.074727</td>\n",
       "      <td>0.156328</td>\n",
       "      <td>-0.002270</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.053267</td>\n",
       "      <td>-0.024458</td>\n",
       "      <td>0.010894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.167736</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>-0.050346</td>\n",
       "      <td>-0.068207</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>-0.099023</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101234</td>\n",
       "      <td>-0.122746</td>\n",
       "      <td>-0.164072</td>\n",
       "      <td>-0.083375</td>\n",
       "      <td>0.152789</td>\n",
       "      <td>-0.002602</td>\n",
       "      <td>-0.068090</td>\n",
       "      <td>0.052757</td>\n",
       "      <td>-0.025889</td>\n",
       "      <td>0.010276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.187541</td>\n",
       "      <td>0.034077</td>\n",
       "      <td>0.020405</td>\n",
       "      <td>-0.053000</td>\n",
       "      <td>-0.070302</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>-0.143792</td>\n",
       "      <td>-0.023337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095281</td>\n",
       "      <td>-0.112873</td>\n",
       "      <td>-0.160837</td>\n",
       "      <td>-0.089385</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>-0.004170</td>\n",
       "      <td>-0.073137</td>\n",
       "      <td>0.070375</td>\n",
       "      <td>-0.009292</td>\n",
       "      <td>-0.001442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.170956</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>0.056115</td>\n",
       "      <td>-0.038492</td>\n",
       "      <td>-0.029014</td>\n",
       "      <td>0.027652</td>\n",
       "      <td>-0.067598</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123382</td>\n",
       "      <td>-0.119782</td>\n",
       "      <td>-0.147127</td>\n",
       "      <td>-0.050388</td>\n",
       "      <td>0.178847</td>\n",
       "      <td>-0.018754</td>\n",
       "      <td>-0.035623</td>\n",
       "      <td>0.030201</td>\n",
       "      <td>-0.054104</td>\n",
       "      <td>0.021888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.065511</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.175231</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>0.027261</td>\n",
       "      <td>-0.051906</td>\n",
       "      <td>-0.057597</td>\n",
       "      <td>0.029128</td>\n",
       "      <td>-0.127557</td>\n",
       "      <td>-0.019282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102296</td>\n",
       "      <td>-0.109977</td>\n",
       "      <td>-0.152842</td>\n",
       "      <td>-0.082822</td>\n",
       "      <td>0.162462</td>\n",
       "      <td>-0.003402</td>\n",
       "      <td>-0.064975</td>\n",
       "      <td>0.061535</td>\n",
       "      <td>-0.018587</td>\n",
       "      <td>0.002062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>0.043827</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.172663</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.038743</td>\n",
       "      <td>-0.041479</td>\n",
       "      <td>-0.054587</td>\n",
       "      <td>0.023998</td>\n",
       "      <td>-0.092068</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104771</td>\n",
       "      <td>-0.124421</td>\n",
       "      <td>-0.149143</td>\n",
       "      <td>-0.070281</td>\n",
       "      <td>0.158868</td>\n",
       "      <td>-0.005580</td>\n",
       "      <td>-0.053896</td>\n",
       "      <td>0.044155</td>\n",
       "      <td>-0.032536</td>\n",
       "      <td>0.011495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.057592</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>0.171603</td>\n",
       "      <td>0.039665</td>\n",
       "      <td>0.021392</td>\n",
       "      <td>-0.051225</td>\n",
       "      <td>-0.056049</td>\n",
       "      <td>0.028933</td>\n",
       "      <td>-0.114749</td>\n",
       "      <td>-0.009119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101970</td>\n",
       "      <td>-0.107772</td>\n",
       "      <td>-0.158038</td>\n",
       "      <td>-0.079672</td>\n",
       "      <td>0.162579</td>\n",
       "      <td>-0.007063</td>\n",
       "      <td>-0.064790</td>\n",
       "      <td>0.056520</td>\n",
       "      <td>-0.023902</td>\n",
       "      <td>0.006668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "6    0.071635  0.000736  0.185331  0.038864  0.023363 -0.053904 -0.064196   \n",
       "16   0.059997 -0.004085  0.173721  0.044702  0.013837 -0.051654 -0.065923   \n",
       "22   0.054114 -0.000525  0.185281  0.043389  0.029967 -0.050980 -0.054634   \n",
       "52   0.056961 -0.003406  0.171711  0.048147  0.025810 -0.050351 -0.070991   \n",
       "76   0.055672 -0.001895  0.167736  0.049130  0.015671 -0.050346 -0.068207   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "805  0.081931  0.001104  0.187541  0.034077  0.020405 -0.053000 -0.070302   \n",
       "816  0.015519  0.004532  0.170956  0.052587  0.056115 -0.038492 -0.029014   \n",
       "845  0.065511  0.002431  0.175231  0.036277  0.027261 -0.051906 -0.057597   \n",
       "846  0.043827  0.004477  0.172663  0.049064  0.038743 -0.041479 -0.054587   \n",
       "879  0.057592 -0.000727  0.171603  0.039665  0.021392 -0.051225 -0.056049   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "6    0.027372 -0.132629 -0.014867  ...  0.100373 -0.114019 -0.163803   \n",
       "16   0.023795 -0.110264 -0.003332  ...  0.096251 -0.113163 -0.170624   \n",
       "22   0.027988 -0.114730 -0.004722  ...  0.109594 -0.115894 -0.168658   \n",
       "52   0.018756 -0.100210 -0.007556  ...  0.104847 -0.130948 -0.164393   \n",
       "76   0.025707 -0.099023 -0.005050  ...  0.101234 -0.122746 -0.164072   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "805  0.027823 -0.143792 -0.023337  ...  0.095281 -0.112873 -0.160837   \n",
       "816  0.027652 -0.067598  0.015114  ...  0.123382 -0.119782 -0.147127   \n",
       "845  0.029128 -0.127557 -0.019282  ...  0.102296 -0.109977 -0.152842   \n",
       "846  0.023998 -0.092068 -0.002324  ...  0.104771 -0.124421 -0.149143   \n",
       "879  0.028933 -0.114749 -0.009119  ...  0.101970 -0.107772 -0.158038   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "6   -0.085835  0.169530 -0.007652 -0.070269  0.064188 -0.017544  0.005122  \n",
       "16  -0.080046  0.159477 -0.006373 -0.070414  0.057019 -0.022347  0.010198  \n",
       "22  -0.075294  0.177010 -0.008734 -0.062928  0.056566 -0.029788  0.010018  \n",
       "52  -0.074727  0.156328 -0.002270 -0.065918  0.053267 -0.024458  0.010894  \n",
       "76  -0.083375  0.152789 -0.002602 -0.068090  0.052757 -0.025889  0.010276  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "805 -0.089385  0.167738 -0.004170 -0.073137  0.070375 -0.009292 -0.001442  \n",
       "816 -0.050388  0.178847 -0.018754 -0.035623  0.030201 -0.054104  0.021888  \n",
       "845 -0.082822  0.162462 -0.003402 -0.064975  0.061535 -0.018587  0.002062  \n",
       "846 -0.070281  0.158868 -0.005580 -0.053896  0.044155 -0.032536  0.011495  \n",
       "879 -0.079672  0.162579 -0.007063 -0.064790  0.056520 -0.023902  0.006668  \n",
       "\n",
       "[63 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_vector = text_vector.apply(pd.Series)\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the vector value from 0 to 1\n",
    "for x in range(len(text_vector)):\n",
    "    x_min = text_vector.iloc[x].min()\n",
    "    x_max = text_vector.iloc[x].max()\n",
    "    X  = text_vector.iloc[x]\n",
    "    i = 0\n",
    "    if (x_max - x_min) == 0:\n",
    "        for y in X:\n",
    "            text_vector.iloc[x][i] = (1/len(text_vector.iloc[x]))\n",
    "            i = i + 1\n",
    "    else:\n",
    "        for y in X:\n",
    "            text_vector.iloc[x][i] = ((y - x_min)/(x_max - x_min))\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.564634</td>\n",
       "      <td>0.401859</td>\n",
       "      <td>0.825668</td>\n",
       "      <td>0.489396</td>\n",
       "      <td>0.453808</td>\n",
       "      <td>0.276413</td>\n",
       "      <td>0.252783</td>\n",
       "      <td>0.463012</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.366037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630614</td>\n",
       "      <td>0.138394</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.203102</td>\n",
       "      <td>0.789390</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.238840</td>\n",
       "      <td>0.547537</td>\n",
       "      <td>0.359891</td>\n",
       "      <td>0.411929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.559111</td>\n",
       "      <td>0.409350</td>\n",
       "      <td>0.824885</td>\n",
       "      <td>0.523365</td>\n",
       "      <td>0.451234</td>\n",
       "      <td>0.298179</td>\n",
       "      <td>0.264833</td>\n",
       "      <td>0.474507</td>\n",
       "      <td>0.161206</td>\n",
       "      <td>0.411108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.154431</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.791598</td>\n",
       "      <td>0.404002</td>\n",
       "      <td>0.254337</td>\n",
       "      <td>0.552151</td>\n",
       "      <td>0.366671</td>\n",
       "      <td>0.442729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.541611</td>\n",
       "      <td>0.417591</td>\n",
       "      <td>0.839337</td>\n",
       "      <td>0.517266</td>\n",
       "      <td>0.486801</td>\n",
       "      <td>0.303066</td>\n",
       "      <td>0.294773</td>\n",
       "      <td>0.482308</td>\n",
       "      <td>0.158365</td>\n",
       "      <td>0.408064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667540</td>\n",
       "      <td>0.155724</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.820562</td>\n",
       "      <td>0.398957</td>\n",
       "      <td>0.275947</td>\n",
       "      <td>0.547175</td>\n",
       "      <td>0.351169</td>\n",
       "      <td>0.441520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.556413</td>\n",
       "      <td>0.418686</td>\n",
       "      <td>0.818215</td>\n",
       "      <td>0.536303</td>\n",
       "      <td>0.485340</td>\n",
       "      <td>0.311581</td>\n",
       "      <td>0.264490</td>\n",
       "      <td>0.469249</td>\n",
       "      <td>0.197827</td>\n",
       "      <td>0.409217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665663</td>\n",
       "      <td>0.127699</td>\n",
       "      <td>0.051394</td>\n",
       "      <td>0.255966</td>\n",
       "      <td>0.783119</td>\n",
       "      <td>0.421277</td>\n",
       "      <td>0.276064</td>\n",
       "      <td>0.547984</td>\n",
       "      <td>0.370655</td>\n",
       "      <td>0.451311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.562203</td>\n",
       "      <td>0.426226</td>\n",
       "      <td>0.826911</td>\n",
       "      <td>0.546751</td>\n",
       "      <td>0.467717</td>\n",
       "      <td>0.311779</td>\n",
       "      <td>0.269590</td>\n",
       "      <td>0.491424</td>\n",
       "      <td>0.196799</td>\n",
       "      <td>0.418773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669826</td>\n",
       "      <td>0.140763</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>0.233761</td>\n",
       "      <td>0.791604</td>\n",
       "      <td>0.424556</td>\n",
       "      <td>0.269866</td>\n",
       "      <td>0.555318</td>\n",
       "      <td>0.369549</td>\n",
       "      <td>0.454975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.584414</td>\n",
       "      <td>0.407009</td>\n",
       "      <td>0.816217</td>\n",
       "      <td>0.479381</td>\n",
       "      <td>0.449371</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.250280</td>\n",
       "      <td>0.465653</td>\n",
       "      <td>0.088979</td>\n",
       "      <td>0.353364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613717</td>\n",
       "      <td>0.156843</td>\n",
       "      <td>0.051566</td>\n",
       "      <td>0.208397</td>\n",
       "      <td>0.772751</td>\n",
       "      <td>0.395432</td>\n",
       "      <td>0.244058</td>\n",
       "      <td>0.559051</td>\n",
       "      <td>0.384191</td>\n",
       "      <td>0.401420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.504649</td>\n",
       "      <td>0.478717</td>\n",
       "      <td>0.871489</td>\n",
       "      <td>0.592131</td>\n",
       "      <td>0.600456</td>\n",
       "      <td>0.377178</td>\n",
       "      <td>0.399547</td>\n",
       "      <td>0.533283</td>\n",
       "      <td>0.308487</td>\n",
       "      <td>0.503691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759211</td>\n",
       "      <td>0.185329</td>\n",
       "      <td>0.120794</td>\n",
       "      <td>0.349103</td>\n",
       "      <td>0.890112</td>\n",
       "      <td>0.423762</td>\n",
       "      <td>0.383950</td>\n",
       "      <td>0.539299</td>\n",
       "      <td>0.340334</td>\n",
       "      <td>0.519679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.562123</td>\n",
       "      <td>0.406260</td>\n",
       "      <td>0.833228</td>\n",
       "      <td>0.489890</td>\n",
       "      <td>0.467613</td>\n",
       "      <td>0.272001</td>\n",
       "      <td>0.257937</td>\n",
       "      <td>0.472225</td>\n",
       "      <td>0.085075</td>\n",
       "      <td>0.352610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653014</td>\n",
       "      <td>0.128514</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.195611</td>\n",
       "      <td>0.801677</td>\n",
       "      <td>0.391847</td>\n",
       "      <td>0.239708</td>\n",
       "      <td>0.552298</td>\n",
       "      <td>0.354327</td>\n",
       "      <td>0.405348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>0.526863</td>\n",
       "      <td>0.434524</td>\n",
       "      <td>0.829189</td>\n",
       "      <td>0.539152</td>\n",
       "      <td>0.514931</td>\n",
       "      <td>0.326684</td>\n",
       "      <td>0.295924</td>\n",
       "      <td>0.480331</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.418565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669874</td>\n",
       "      <td>0.132052</td>\n",
       "      <td>0.074039</td>\n",
       "      <td>0.259097</td>\n",
       "      <td>0.796818</td>\n",
       "      <td>0.410924</td>\n",
       "      <td>0.297546</td>\n",
       "      <td>0.527631</td>\n",
       "      <td>0.347670</td>\n",
       "      <td>0.450993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>0.556141</td>\n",
       "      <td>0.412015</td>\n",
       "      <td>0.837902</td>\n",
       "      <td>0.511839</td>\n",
       "      <td>0.466680</td>\n",
       "      <td>0.287219</td>\n",
       "      <td>0.275297</td>\n",
       "      <td>0.485317</td>\n",
       "      <td>0.130227</td>\n",
       "      <td>0.391276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665815</td>\n",
       "      <td>0.147472</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>0.216916</td>\n",
       "      <td>0.815602</td>\n",
       "      <td>0.396358</td>\n",
       "      <td>0.253695</td>\n",
       "      <td>0.553494</td>\n",
       "      <td>0.354743</td>\n",
       "      <td>0.430290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "6    0.564634  0.401859  0.825668  0.489396  0.453808  0.276413  0.252783   \n",
       "16   0.559111  0.409350  0.824885  0.523365  0.451234  0.298179  0.264833   \n",
       "22   0.541611  0.417591  0.839337  0.517266  0.486801  0.303066  0.294773   \n",
       "52   0.556413  0.418686  0.818215  0.536303  0.485340  0.311581  0.264490   \n",
       "76   0.562203  0.426226  0.826911  0.546751  0.467717  0.311779  0.269590   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "805  0.584414  0.407009  0.816217  0.479381  0.449371  0.288256  0.250280   \n",
       "816  0.504649  0.478717  0.871489  0.592131  0.600456  0.377178  0.399547   \n",
       "845  0.562123  0.406260  0.833228  0.489890  0.467613  0.272001  0.257937   \n",
       "846  0.526863  0.434524  0.829189  0.539152  0.514931  0.326684  0.295924   \n",
       "879  0.556141  0.412015  0.837902  0.511839  0.466680  0.287219  0.275297   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       "6    0.463012  0.095669  0.366037  ...  0.630614  0.138394  0.024098   \n",
       "16   0.474507  0.161206  0.411108  ...  0.643836  0.154431  0.020144   \n",
       "22   0.482308  0.158365  0.408064  ...  0.667540  0.155724  0.035959   \n",
       "52   0.469249  0.197827  0.409217  ...  0.665663  0.127699  0.051394   \n",
       "76   0.491424  0.196799  0.418773  ...  0.669826  0.140763  0.043147   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "805  0.465653  0.088979  0.353364  ...  0.613717  0.156843  0.051566   \n",
       "816  0.533283  0.308487  0.503691  ...  0.759211  0.185329  0.120794   \n",
       "845  0.472225  0.085075  0.352610  ...  0.653014  0.128514  0.022600   \n",
       "846  0.480331  0.207971  0.418565  ...  0.669874  0.132052  0.074039   \n",
       "879  0.485317  0.130227  0.391276  ...  0.665815  0.147472  0.023246   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "6    0.203102  0.789390  0.382600  0.238840  0.547537  0.359891  0.411929  \n",
       "16   0.231828  0.791598  0.404002  0.254337  0.552151  0.366671  0.442729  \n",
       "22   0.247878  0.820562  0.398957  0.275947  0.547175  0.351169  0.441520  \n",
       "52   0.255966  0.783119  0.421277  0.276064  0.547984  0.370655  0.451311  \n",
       "76   0.233761  0.791604  0.424556  0.269866  0.555318  0.369549  0.454975  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "805  0.208397  0.772751  0.395432  0.244058  0.559051  0.384191  0.401420  \n",
       "816  0.349103  0.890112  0.423762  0.383950  0.539299  0.340334  0.519679  \n",
       "845  0.195611  0.801677  0.391847  0.239708  0.552298  0.354327  0.405348  \n",
       "846  0.259097  0.796818  0.410924  0.297546  0.527631  0.347670  0.450993  \n",
       "879  0.216916  0.815602  0.396358  0.253695  0.553494  0.354743  0.430290  \n",
       "\n",
       "[63 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The average silhouette_score is : 0.4614092\n",
      "For n_clusters = 3 The average silhouette_score is : 0.3122708\n",
      "For n_clusters = 4 The average silhouette_score is : 0.3198335\n",
      "For n_clusters = 5 The average silhouette_score is : 0.3057836\n",
      "For n_clusters = 6 The average silhouette_score is : 0.3259043\n",
      "For n_clusters = 7 The average silhouette_score is : 0.29711112\n",
      "For n_clusters = 8 The average silhouette_score is : 0.27386585\n",
      "For n_clusters = 9 The average silhouette_score is : 0.2710567\n",
      "For n_clusters = 10 The average silhouette_score is : 0.2667427\n",
      "For n_clusters = 11 The average silhouette_score is : 0.27484012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score,silhouette_samples\n",
    "range_n_clusters = [2,3,4, 5, 6, 7, 8, 9, 10, 11]\n",
    "X = text_vector\n",
    "n_best_clusters = 0\n",
    "silhouette_best = 0\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer=KMeans(n_clusters=n_clusters,random_state=10)\n",
    "    cluster_labels=clusterer.fit_predict(X)\n",
    "    silhouette_avg=silhouette_score(X,cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if silhouette_avg > silhouette_best:\n",
    "        silhouette_best = silhouette_avg\n",
    "        n_best_clusters = n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_best_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters= n_best_clusters , random_state=10)\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cluster_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of text, the corresponding cluster number\n",
    "finaldf = pd.DataFrame({'cl_num': cluster_labels,'text_body': df['text_body'], 'tokenize_text': df['tokenize_text'], 'abstract': df['abstract'],'title': df['title']})\n",
    "finaldf = finaldf.sort_values(by=['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['cl_num'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clusters=np.unique(df['cl_num'])\n",
    "final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_select='text_body'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      0\n",
       "16     0\n",
       "22     0\n",
       "52     0\n",
       "76     0\n",
       "      ..\n",
       "805    0\n",
       "816    1\n",
       "845    0\n",
       "846    0\n",
       "879    0\n",
       "Name: cl_num, Length: 63, dtype: int32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cl_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name=np.unique(df['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all text phrase corresponding to each cluster in a file.\n",
    "for i in final_clusters:\n",
    "    with open('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/treat_Cluster_'+str(i)+'.txt','w',encoding='utf-8') as out:\n",
    "        y = ''\n",
    "        for x in df[text_select][df.cl_num == i]:    \n",
    "            y = y + x + '. '\n",
    "        out.write(y)\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "utf_8_encode() argument 1 must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-8fab6be0f79c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mphrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, object)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\codecs.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, object)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\" Writes the object's contents encoded to self.stream.\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: utf_8_encode() argument 1 must be str, not bytes"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "phrases = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n",
    "\n",
    "\n",
    "A = '(CD|JJ)/\\w+\\s'  #cd or jj\n",
    "B = '(NN|NNS|NNP|NNPS)/\\w+\\s'  #nouns\n",
    "C = '(VB|VBD|VBG|VBN|VBP|VBZ)/\\w+\\s' #verbs\n",
    "D = 'FW/\\w+\\s'  #foreign word\n",
    "patterns = ['('+A+B+')+', '('+D+B+')+','('+D+')+', '('+B+')+', '('+D+A+B+')+', \n",
    "           '('+B+C+')+', '('+D+B+C+')+', '('+B+A+B+')+', '('+B+B+C+')+'] \n",
    "\n",
    "def extract_phrases(tag1, tag2, sentences):\n",
    "    extract_phrase = []\n",
    "    for sentence in sentences:\n",
    "        phrase = []\n",
    "        next_word = 0\n",
    "        for word, pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if next_word == 1:\n",
    "                next_word = 0\n",
    "                if pos == tag2:\n",
    "                    extract_phrase = np.append(extract_phrase,phrase + ' ' + word) \n",
    "            \n",
    "            if pos == tag1:\n",
    "                next_word = 1\n",
    "                phrase = word\n",
    "    return extract_phrase\n",
    "\n",
    "for i in cluster_name:\n",
    "    File = open('C:/Users/Chandrapal Panwar/Desktop/python/covid-19/treat_Cluster_'+str(i)+'.txt', 'r',encoding='utf-8') #open file\n",
    "    lines = File.read() #read all lines\n",
    "    sentences = nltk.sent_tokenize(lines) #tokenize sentences\n",
    "\n",
    "    for sentence in sentences: \n",
    "        f = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        tag_seq = []\n",
    "        for word, pos in f:\n",
    "            tag_seq.append(pos+'/'+ word)\n",
    "        X = \" \".join(tag_seq)\n",
    "\n",
    "        phrase = []\n",
    "        for j in range(len(patterns)):\n",
    "            if re.search(patterns[j], X):\n",
    "                phrase.append(' '.join([word.split('/')[1] for word in re.search(patterns[j], X).group(0).split()]))\n",
    "    \n",
    "        k = pd.DataFrame({'extracted_phrases': np.unique(phrase), 'cluster_num': int(i)})\n",
    "    \n",
    "        phrases = pd.concat([phrases,k], ignore_index = True)\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
